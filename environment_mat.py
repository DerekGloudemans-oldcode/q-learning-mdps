# import required packages
import numpy as np
import random


########################## Environment_pred Class ##############################
# Environment classes provide functions for maintaining agents, the environment, 
# and the interaction between them
# Environment_pred represents the predator CMDP as discussed in Laetitia et al.
class Environment_mat():
    
    #_________________________Environment_lane()______________________________#
    # initializes a 1-lane bridge environment
    # inputs:
        # num agents (int > 1)
        # agent params - dictionary of agent parameters (e,a,g,method, mod)
    def __init__(self, stochastic, agent_params):
        
        
        # create reward matrix
        self.rewards = np.zeros([4,4])
        self.stochastic = stochastic
        self.rewards[:,0] = np.asarray([10, -10, -10, -10])
        self.rewards[:,1] = np.asarray([-10,  1, 6, 3])
        self.rewards[:,2] = np.asarray([-10,  6, 4, 8])
        self.rewards[:,3] = np.asarray([-10,  3, 8, 4])
        num_actions = 4
        self.num_agents = 2
        
        self.moves = [] # will store the moves so reward knows them
        self.agents = []
        for i in range (0,self.num_agents):
            agent = {}
            agent['sa_vals'] = np.zeros([4])-10
            agent['params'] = agent_params
            self.agents.append(agent)

        
    #_________________________get_start_state()______________________________#
    # does nothing as there is only a single state, implemented so this 
    # environment can interact with the Simulator class
    def get_start_state(self):
        self.moves = [0, 0]


    #_________________________get_next_state()______________________________#
    # does nothing as there is only a single state, implemented so this 
    # environment can interact with the Simulator class
    def get_next_state(self, actions): 
        self.moves[0] = actions[0]
        self.moves[1] = actions[1]

        
                
                
    #____________________________get_reward()_________________________________#
    # gets reward for a pair of actions
    # returns: reward (int)
    def get_reward(self):
        reward = self.rewards[self.moves[0],self.moves[1]]
        
        # for actions [1,1] reward of 100 with probability 0.01
        if self.stochastic and reward == 1:
            if random.random() < 0.01:
                reward = 100
                
        return reward
    
    #_________________________action_selection ()______________________________#
    # selects an action for one agent according to epsilon-greedy policy
    # inputs: agent_num (int)
    # returns: move (int 0 or 1)
    def action_selection(self,agent_num): #Q-learning and variants, SARSA and variants supported
        
        epsilon = self.agents[agent_num]['params']['epsilon']
        sa_vals = self.agents[agent_num]['sa_vals']
        
        # if rand < epsilon select random move
        rand = random.random()
        if rand < epsilon:
            move = random.randint(0,3)
            
        # else select best move
        else: 
            max_val = -1* np.inf
            max_move = 0
            vals = [0,1,2,3]
            random.shuffle(vals)
            for i in vals:
                if sa_vals[i] > max_val:
                    max_val = sa_vals[i]
                    max_move = i
            move = max_move
        
        return move, None # the None is the state_num in other environments
        
    #____________________________update_values()______________________________#
    # compare previously predicted value to new estimate of value and performs 
    # update simultaneously for all agents
    # inputs:
        # moves - 1 x num_agents list of moves (ints from 0-4) 
        # reward - the reward generated by moving to the current state
        # prev_state_num - unique tag specifying previous state (int)
    def update_values(self, moves, reward, prev_state_nums):
        
        next_moves = []
        # for one agent
        for i in range(0, self.num_agents):
            
            # store agent parameters in temp variables
            epsilon = self.agents[i]['params']['epsilon']
            alpha = self.agents[i]['params']['alpha']
            gamma = self.agents[i]['params']['gamma']
            method = self.agents[i]['params']['method'] # Q 
            mod = self.agents[i]['params']['mod'] # None, distributed or hysteretic
            
            sa_vals = self.agents[i]['sa_vals']
            
            # select next move
            rand = random.random()
            # use current epsilon-greedy policy if SARSA
            if method == 'SARSA' and rand < epsilon:
                    next_move = random.randint(0,3)       
            # else select best move
            else:
                max_val = -1* np.inf
                max_move = 0
                vals = [0,1,2,3]
                random.shuffle(vals)
                for j in vals:
                    if sa_vals[j] > max_val:
                        max_val = sa_vals[j]
                        max_move = j
                next_move = max_move
                
            #get update sa_val for agents
            update_val = reward 
            
            # get difference between old and new values
            prev_val = sa_vals[moves[i]]
            diff = update_val - prev_val
            
            # add distributed and hysteretic functionality here
            if mod == 'distributed':
                if diff < 0:
                    alpha = 0
            elif mod == 'hysteretic':
                if diff < 0:
                    alpha = 0.1 * alpha
                    
            self.agents[i]['sa_vals'][moves[i]] = (1-alpha)*prev_val + alpha*(update_val)
            next_moves.append(next_move)
        return next_moves
  
     